# LlmCliTool: Cooperative Remote Command Execution with LLM Guidance

This project extends the `PanelCliExample` by enabling users to cooperatively execute SSH commands on a remote server with guidance from a Large Language Model (LLM). The setup uses Docker to manage a Panel web interface where both user and LLM can submit commands.

## Features

- **Interactive Web Interface**: A Panel-based GUI for executing and viewing command outputs.
- **Cooperative Command Execution**: Users can leverage suggestions and directly execute commands generated by an LLM.
- **Docker Integration**: Provides a consistent and safe environment for both the Panel app and target SSH server.

## Disclaimer

**Critical Security Warning:** Enabling LLMs to execute commands directly poses significant security risks. It is essential to acknowledge the potential vulnerabilities:

- **LLM-Generated Commands**: The LLM can generate arbitrary commands that are executed in the shell without prior approval, which may inadvertently lead to malicious actions.
- **Shell Access Risks**: Access to the shell, even within a containerized environment, can expose the system to various security threats.
- **Privilege Escalation and Container Escape**: Though Docker offers isolation, the potential for privilege escalation and container escape exists.

### Suggested Security Enhancements

1. **Cap Limits**: Configure container capabilities to restrict LLM operations. Use Docker's `--cap-drop` and `--cap-add` options to selectively disable/enable Linux kernel capabilities.

2. **Resource Limits**: Use Docker resource constraints (`--memory`, `--cpus`) to prevent excessive use of host resources.

3. **Monitoring and Logging**: Continuously monitor command logs to detect suspicious activity. Consider using intrusion detection systems within your containerized setup.

4. **Non-Root User**: Run all processes as a non-root user inside the container to minimize the potential for privilege escalation.

5. **Review and Validate Commands**: Implement a validation step where commands suggested by the LLM are reviewed by a human before execution.

## Prerequisites

Ensure you have the following installed:

- **Docker**: [Install Docker](https://docs.docker.com/get-docker/)
- **Docker Compose**: [Install Docker Compose](https://docs.docker.com/compose/install/)
- **OpenAI API Key**: Obtain an API key from OpenAI and set it in your environment.

## Setup Guide

### 1. Clone the Repository

```bash
git clone https://github.com/stepbot/LearnDockerLLM.git
cd LearnDockerLLM/LlmCliTool
```

### 2. Create and Configure Environment File

Create a `.env` file in the `panel_app` directory. Add your OpenAI API key along with SSH configuration:

```plaintext
OPENAI_API_KEY=your-openai-api-key
TARGET_HOST=target
TARGET_SSH_USER=testuser
TARGET_SSH_PASS=password
TARGET_SSH_PORT=22
```

### 3. Build and Run the Application

Build and start the application using Docker Compose:

```bash
docker-compose up --build
```

### 4. Access the Web Interface

The application is available at [http://localhost:5006](http://localhost:5006). Interact with the interface to execute commands and view LLM suggestions.

## Component Breakdown

- **`app.py`**: Core logic for managing command execution and interaction with the LLM and SSH system.
- **`compose.yml`**: Configuration for Docker Compose, detailing dependencies, network settings, and environmental variables for services.

## Development

For development or modification, adjust the `app.py` file. Rebuild containers after changes using:

```bash
docker-compose up --build
```

## Contributing

Contributions are welcome! Please fork the repository, implement your changes, and create a pull request.

## License

This project is licensed under the [MIT License](../license.txt).

## Contact

If you have questions or feedback, please open an issue on the repository.